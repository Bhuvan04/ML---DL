Decision Tree: (Entropy or gini indexing is for categorical features and for continuous features we can use RSS to select the route node)
  Entropy of all the columns : E = -Pi * log(Pi)
    Pi = When Target value is a then for how many values of column x with 1 value occours
  Information Gain,  It is the difference between entropies before and after the split : G = (1-submission(Sv/S)Ei)
    Sv is the occurances and S is total number of values
  Gini Indexing : (1-submission(Pi^2)) - Route node will start with the lowest gini index column

Ensemble techniques - Used to reduce variance, bias and improve prediction :
  Bagging - Each tree given equal wt. eg. Random Forest
  Boosting - Trees given unequal wt. Using RSS value we can choose the route node for the boosting algo's
    Ada Boost (Adaptive Boosting) - In this we have contribution(Alfa) & wt.
    Gradient Boost - In this we have residuals and learning rate.
    XGBOOST & XGBClassifier (Xtreme Gradient Boost)--> Best algo - using hessian(No. of residuals) which is second order derivative of loss function, here gamma used for tunning
      It talks about memory & Cache optimization and do the parallel processing(n_jobs = -1, for full cpu power use). It does greedy approach.
  Stacking - We can use different models for the same dataset but in begging and boosting we used to create only single model.
  Pasting: without replacement
  
KNN (Lazy Learner)- Is used for supervised learning (for small dataset it is good but not for large dataset). We should use K value as odd number.
  Euclidean Distance - For continuous distance
  Hamming Distance - For catagorical distance : if A=A, then 0 else 1
  Manhattan Distance - Absolute distance calculation.
  
K-mean is used for unsupervised learning

VIF: Can tell whether there is any multicollinearity present. If VIF>5 then yes there is multicollinearity.
PCA: Can tell us the data loss or information loss

To find whether our model is good or not (Mostly calulated using confusion matrix) : ROC, AOC, Presision, recall, acurracy, score, Elbow method

Supervised : we have X & Y datasets
Unsupervised : we have only X dataset

Find the shape of the dataset and then we can apply the perticular type of clusturing approach. DBSCAN (Density based clusturing) is best. 
With the help of WCSS and Elbow method we try to find the value of K

Stacking and XGBoost are the best algorithms.

PCA (In Sklearn.decomposition): Using Scree plot we can see how many PCA components can explain the maximum variance and can choose those many components only.
  We are having some information loss. It is only for continuous values
  MFA (Multifactor Analysis): Takes care of both continuous and categorical values.
  LDA performs better than PCA. PCA is unsupervised and LDA is supervised approach. PCA is faster than LDA
  


