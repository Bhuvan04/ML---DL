Decision Tree: (Entropy is for categorical features and for continuous features we can use gini indexing)
  Entropy of all the columns : E = -Pi * log(Pi)
    Pi = When Target value is a then for how many values of column x with 1 value occours
  Information Gain : G = (1-submission(Sv/S)Ei)
    Sv is the occurances and S is total number of values
  Gini Indexing : (1-submission(Pi^2))

Bagging - Each tree given equal wt. eg. Random Forest
Boosting - Trees given unequal wt.

XGBOOST --> Best algo
