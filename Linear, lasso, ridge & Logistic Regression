Common machine learning algorithms for regression problems include:
  Linear Regression : Fast training, linear model
  Decision Forest Regression : Accurate, fast training times
  Neural Net Regression : Accurate, long training times

Gradient Desent : A method that minimizes the amount of error at each step of the model training process.

Linear Regression ((Ordinary Least Square Method) & (Gradient Desent)) Model Evaluation: 
  Root Mean Squared Error (RMSE) : Square root of the squared differences between the predicted and actual values.
  Mean Absolute Error (MAE) : Average of the absolute difference between each prediction and the true value.
  R-Squared : How close the regression line is to the true values.
  Spearman Correlation : Strength and direction of the relationship between predicted and actual values.

Logistic Regression Model evaluation:
  Confusion Matrix: TP, FP, FN, TN
    Accuracy = (TP+TN)/(TP+FP+FN+TN)
    Recall = TP/(TP+FN)
    Precision = TP/(TP+FP)
    F1 Score = 2∗(Precision∗Recall)/(Precision+Recall)
  
  Receiver Operating Characteristics (ROC) curve = TP on Y axis & FP on X axis.

  Area Under the Curve (AUC) = lies b/w 0.5 to 1 ideally. An AUC of 0.5 indicates random guessing, while an AUC of 1.0 indicates perfect classification.
  
  Ridge Regression : Increases Bias but decreases variance. SSR+lamda*(Slope^2)
                     Lamda could vary from 0 to infinity any value.
  
  Lasso Regression : Increases Bias but decreases variance. SSR+lamda*(|Slope|)

https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229
