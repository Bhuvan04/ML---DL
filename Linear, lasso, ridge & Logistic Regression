Methods to avoid Over-fitting: Following are the commonly used methodologies :

1. Cross-Validation : Cross Validation in its simplest form is a one round validation, where we leave one sample as in-time validation and rest for training the model.
                   But for keeping lower variance a higher fold cross validation is preferred.
2. Early Stopping : Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit.
3. Pruning : Pruning is used extensively while building CART models. It simply removes the nodes which add little predictive power for the problem in hand.
4. Regularization : This is the technique we are going to discuss in more details. Simply put, it introduces a cost term for bringing in more features with the 
                   objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term.

Common machine learning algorithms for regression problems include:
  Linear Regression : Fast training, linear model
  Decision Forest Regression : Accurate, fast training times
  Neural Net Regression : Accurate, long training times

Gradient Desent : A method that minimizes the amount of error at each step of the model training process.

Linear Regression ((Ordinary Least Square Method) & (Gradient Desent)) Model Evaluation: 
  Root Mean Squared Error (RMSE) : Square root of the squared differences between the predicted and actual values.
  Mean Absolute Error (MAE) : Average of the absolute difference between each prediction and the true value.
  R-Squared : How close the regression line is to the true values.
  Spearman Correlation : Strength and direction of the relationship between predicted and actual values.

Logistic Regression Model evaluation: odds = p/(1-p) => log((p)/(1-p)) = b + b1x1 + b2x2 ...
  Confusion Matrix: TP, FP, FN, TN
    Accuracy = (TP+TN)/(TP+FP+FN+TN)
    Recall = TP/(TP+FN)
    Precision = TP/(TP+FP)
    F1 Score = 2∗(Precision∗Recall)/(Precision+Recall)
  
  Receiver Operating Characteristics (ROC) curve = TP on Y axis & FP on X axis.

  Area Under the Curve (AUC) = lies b/w 0.5 to 1 ideally. An AUC of 0.5 indicates random guessing, while an AUC of 1.0 indicates perfect classification.
  
  Ridge Regression : Increases Bias but decreases variance. SSR+lamda*(Slope^2)
                     Lamda could vary from 0 to infinity any value.
  
  Lasso Regression : Increases Bias but decreases variance. SSR+lamda*(|Slope|)

https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229
