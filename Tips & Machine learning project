Python is a high level language which is built on top of C, which is low level language and C is built on top of B, which is the languge of machine.
  High level languges are easy to understand by human and low level are easy to understand by machine.

API: https://www.geeksforgeeks.org/social-network-api/

Enable personalized, real-time banking experiences with chatbots

Identify the next best action for the customer - sentiment analysis

Capture, prioritize, and route service requests to the correct employee, and improve response times - 
  A busy government organization gets innumerable service requests on an annual basis. Machine learning tools can help to capture incoming service requests, 
  to route them to the correct employee in real-time, to refine prioritization, and improve response times. 

Automate the recognition of disease
Recommend next best actions for individual care plans

Standardization rescales data so that it has a mean of 0 and a standard deviation of 1.
  The formula for this is: (𝑥 − 𝜇)/𝜎
 
Normalization rescales the data into the range [0, 1].
  The formula for this is: (𝑥 −𝑥𝑚𝑖𝑛)/(𝑥𝑚𝑎𝑥 −𝑥𝑚𝑖𝑛)
  
There are two common approaches for encoding categorical data: 
  Ordinal encoding: we simply convert the categorical data into integer codes ranging from 0 to (number of categories – 1).
    One of the potential drawbacks to this approach is that it implicitly assumes an order across the categories.
  One hot encoding: we transform each categorical value into a column. If there are n categorical values, n new columns are added.
    One drawback of one-hot encoding is that it can potentially generate a very large number of columns.
 
Image Data type : The number of channels required to represent the color is known as the color depth or simply depth. With an RGB image, depth = 3, 
                  because there are three channels (Red, Green, and Blue). In contrast, a grayscale image has depth = 1, because there is only one channel.
    Encoding an Image: We need to know the following three things about an image to reproduce it:
                       Horizontal position of each pixel
                       Vertical position of each pixel
                       Color of each pixel
        Thus, we can fully encode an image numerically by using a vector with three dimensions. The size of the vector required for any given image would be the 
                        height * width * depth of that image.

Text Data Type: Text normalization is the process of transforming a piece of text into a canonical (official) form.
                1. Lemmatization is an example of normalization. A lemma is the dictionary form of a word and lemmatization is the process of reducing multiple inflections 
                to that single dictionary form. 
                2. Stop words are high-frequency words that are unnecessary (or unwanted) during the analysis, you may want to remove stop words.
                   Tokenized the text (i.e. split each string of text into a list of smaller parts or tokens)

Linear Regression:
ML is adjusting the parameters in such a way that error value comes out to be minimum. We can achieve it by Gradient descent. 
  Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
  Model used to change the learning rate to get the optimum value but we don't have control to change that.
  The process of finding the best model is essentially a process of finding the coefficients and bias that minimize this error. we use a cost function.
  The most commonly used cost function for linear regression is the root mean squared error (RMSE)
  
    
    
Column can also be called as features or attributes
Row can also be called as entity, instance or input vector

The Trade-Offs
  Two of the most important are bias vs. variance and overfitting vs. underfitting.
  
  Bias measures how inaccurate the model prediction is in comparison with the true output. It is due to erroneous assumptions made in the machine learning process to simplify
    the model and make the target function easier to learn. High model complexity tends to have a low bias.
  Variance measures how much the target function will change if different training data is used. Variance can be caused by modeling the random noise in the training data. 
    High model complexity tends to have a high variance.

  As a general trend, parametric and linear algorithms often have high bias and low variance, whereas non-parametric and non-linear algorithms often have low bias 
    and high variance.
    
  Overfitting refers to the situation in which models fit the training data very well, but fail to generalize to new data.

  Underfitting refers to the situation in which models neither fit the training data nor generalize to new data.

The prediction error can be viewed as the sum of model error (error coming from the model) and the irreducible error (coming from data collection).
    prediction error = Bias error + variance + error + irreducible error
    
low bias are KNN and decision trees
High bias & low variance is in Linear regression
Support vector machines usually have a high variance

k-fold cross-validation: it split the initial training data into k subsets and train the model k times. In each training, it uses one subset as the testing data and 
                         the rest as training data. This is to reduce Overfitting.

Feature Engineering: We can improve the situation of having too many features through dimensionality reduction.
  1. PCA (Principal Component Analysis)
  2. t-SNE (t-Distributed Stochastic Neighboring Entities)
  3. Feature embedding
